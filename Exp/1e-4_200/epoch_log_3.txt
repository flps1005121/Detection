Total training samples: 22
Epoch [1/200], Loss: 1.2589
Epoch [2/200], Loss: 0.8631
Epoch [3/200], Loss: 0.7088
Epoch [4/200], Loss: 0.6112
Epoch [5/200], Loss: 0.7042
Epoch [6/200], Loss: 0.7885
Epoch [7/200], Loss: 0.6451
Epoch [8/200], Loss: 0.6811
Epoch [9/200], Loss: 0.7723
Epoch [10/200], Loss: 0.7499
Epoch [11/200], Loss: 0.7235
Epoch [12/200], Loss: 0.5729
Epoch [13/200], Loss: 0.6399
Epoch [14/200], Loss: 0.7308
Epoch [15/200], Loss: 0.6914
Epoch [16/200], Loss: 0.4005
Epoch [17/200], Loss: 0.6465
Epoch [18/200], Loss: 0.6815
Epoch [19/200], Loss: 0.5086
Epoch [20/200], Loss: 0.7750
Epoch [21/200], Loss: 0.5647
Epoch [22/200], Loss: 0.7962
Epoch [23/200], Loss: 0.5586
Epoch [24/200], Loss: 0.5551
Epoch [25/200], Loss: 0.5747
Epoch [26/200], Loss: 0.6010
Epoch [27/200], Loss: 0.6031
Epoch [28/200], Loss: 0.4684
Epoch [29/200], Loss: 0.4536
Epoch [30/200], Loss: 0.5229
Epoch [31/200], Loss: 0.5922
Epoch [32/200], Loss: 0.4910
Epoch [33/200], Loss: 0.5618
Epoch [34/200], Loss: 0.9184
Epoch [35/200], Loss: 0.5346
Epoch [36/200], Loss: 0.5237
Epoch [37/200], Loss: 0.5343
Epoch [38/200], Loss: 0.6527
Epoch [39/200], Loss: 0.4932
Epoch [40/200], Loss: 0.4914
Epoch [41/200], Loss: 0.5257
Epoch [42/200], Loss: 0.4919
Epoch [43/200], Loss: 0.5032
Epoch [44/200], Loss: 0.4183
Epoch [45/200], Loss: 0.4410
Epoch [46/200], Loss: 0.6060
Epoch [47/200], Loss: 0.4976
Epoch [48/200], Loss: 0.5788
Epoch [49/200], Loss: 0.6558
Epoch [50/200], Loss: 0.5452
Epoch [51/200], Loss: 0.3834
Epoch [52/200], Loss: 0.6372
Epoch [53/200], Loss: 0.4304
Epoch [54/200], Loss: 0.5390
Epoch [55/200], Loss: 0.4737
Epoch [56/200], Loss: 0.4754
Epoch [57/200], Loss: 0.5853
Epoch [58/200], Loss: 0.4428
Epoch [59/200], Loss: 0.4199
Epoch [60/200], Loss: 0.3882
Epoch [61/200], Loss: 0.5404
Epoch [62/200], Loss: 0.3870
Epoch [63/200], Loss: 0.4025
Epoch [64/200], Loss: 0.4456
Epoch [65/200], Loss: 0.5266
Epoch [66/200], Loss: 0.4148
Epoch [67/200], Loss: 0.4585
Epoch [68/200], Loss: 0.4023
Epoch [69/200], Loss: 0.4775
Epoch [70/200], Loss: 0.5363
Epoch [71/200], Loss: 0.4124
Epoch [72/200], Loss: 0.4534
Epoch [73/200], Loss: 0.5551
Epoch [74/200], Loss: 0.4436
Epoch [75/200], Loss: 0.4257
Epoch [76/200], Loss: 0.4673
Epoch [77/200], Loss: 0.4777
Epoch [78/200], Loss: 0.4108
Epoch [79/200], Loss: 0.5400
Epoch [80/200], Loss: 0.4851
Epoch [81/200], Loss: 0.5116
Epoch [82/200], Loss: 0.4329
Epoch [83/200], Loss: 0.4111
Epoch [84/200], Loss: 0.4762
Epoch [85/200], Loss: 0.4928
Epoch [86/200], Loss: 0.4122
Epoch [87/200], Loss: 0.4957
Epoch [88/200], Loss: 0.4526
Epoch [89/200], Loss: 0.3863
Epoch [90/200], Loss: 0.3956
Epoch [91/200], Loss: 0.4004
Epoch [92/200], Loss: 0.3987
Epoch [93/200], Loss: 0.4528
Epoch [94/200], Loss: 0.3708
Epoch [95/200], Loss: 0.4977
Epoch [96/200], Loss: 0.4042
Epoch [97/200], Loss: 0.5450
Epoch [98/200], Loss: 0.4000
Epoch [99/200], Loss: 0.3624
Epoch [100/200], Loss: 0.4180
Epoch [101/200], Loss: 0.4650
Epoch [102/200], Loss: 0.4048
Epoch [103/200], Loss: 0.3599
Epoch [104/200], Loss: 0.4385
Epoch [105/200], Loss: 0.3731
Epoch [106/200], Loss: 0.5274
Epoch [107/200], Loss: 0.4010
Epoch [108/200], Loss: 0.4495
Epoch [109/200], Loss: 0.3931
Epoch [110/200], Loss: 0.4229
Epoch [111/200], Loss: 0.4520
Epoch [112/200], Loss: 0.3877
Epoch [113/200], Loss: 0.4405
Epoch [114/200], Loss: 0.3517
Epoch [115/200], Loss: 0.4442
Epoch [116/200], Loss: 0.3436
Epoch [117/200], Loss: 0.4344
Epoch [118/200], Loss: 0.4124
Epoch [119/200], Loss: 0.4140
Epoch [120/200], Loss: 0.4059
Epoch [121/200], Loss: 0.4471
Epoch [122/200], Loss: 0.4268
Epoch [123/200], Loss: 0.3963
Epoch [124/200], Loss: 0.4261
Epoch [125/200], Loss: 0.5886
Epoch [126/200], Loss: 0.4241
Epoch [127/200], Loss: 0.3905
Epoch [128/200], Loss: 0.4047
Epoch [129/200], Loss: 0.4061
Epoch [130/200], Loss: 0.3984
Epoch [131/200], Loss: 0.4036
Epoch [132/200], Loss: 0.3811
Epoch [133/200], Loss: 0.3577
Epoch [134/200], Loss: 0.3541
Epoch [135/200], Loss: 0.3493
Epoch [136/200], Loss: 0.3916
Epoch [137/200], Loss: 0.4040
Epoch [138/200], Loss: 0.4091
Epoch [139/200], Loss: 0.3734
Epoch [140/200], Loss: 0.3786
Epoch [141/200], Loss: 0.3940
Epoch [142/200], Loss: 0.3800
Epoch [143/200], Loss: 0.3789
Epoch [144/200], Loss: 0.4162
Epoch [145/200], Loss: 0.3870
Epoch [146/200], Loss: 0.4024
Epoch [147/200], Loss: 0.4667
Epoch [148/200], Loss: 0.3837
Epoch [149/200], Loss: 0.4657
Epoch [150/200], Loss: 0.3452
Epoch [151/200], Loss: 0.4053
Epoch [152/200], Loss: 0.3706
Epoch [153/200], Loss: 0.3316
Epoch [154/200], Loss: 0.3510
Epoch [155/200], Loss: 0.3817
Epoch [156/200], Loss: 0.3865
Epoch [157/200], Loss: 0.4506
Epoch [158/200], Loss: 0.3806
Epoch [159/200], Loss: 0.4212
Epoch [160/200], Loss: 0.4233
Epoch [161/200], Loss: 0.4911
Epoch [162/200], Loss: 0.4337
Epoch [163/200], Loss: 0.3909
Epoch [164/200], Loss: 0.3617
Epoch [165/200], Loss: 0.3627
Epoch [166/200], Loss: 0.4199
Epoch [167/200], Loss: 0.3840
Epoch [168/200], Loss: 0.4093
Epoch [169/200], Loss: 0.3753
Epoch [170/200], Loss: 0.4855
Epoch [171/200], Loss: 0.4170
Epoch [172/200], Loss: 0.5537
Epoch [173/200], Loss: 0.3432
Epoch [174/200], Loss: 0.3869
Epoch [175/200], Loss: 0.5182
Epoch [176/200], Loss: 0.4575
Epoch [177/200], Loss: 0.4117
Epoch [178/200], Loss: 0.3926
Epoch [179/200], Loss: 0.3899
Epoch [180/200], Loss: 0.5487
Epoch [181/200], Loss: 0.4582
Epoch [182/200], Loss: 0.3522
Epoch [183/200], Loss: 0.4193
Epoch [184/200], Loss: 0.4026
Epoch [185/200], Loss: 0.3752
Epoch [186/200], Loss: 0.4304
Epoch [187/200], Loss: 0.4208
Epoch [188/200], Loss: 0.4476
Epoch [189/200], Loss: 0.4300
Epoch [190/200], Loss: 0.4159
Epoch [191/200], Loss: 0.4540
Epoch [192/200], Loss: 0.3738
Epoch [193/200], Loss: 0.4221
Epoch [194/200], Loss: 0.4016
Epoch [195/200], Loss: 0.5258
Epoch [196/200], Loss: 0.3889
Epoch [197/200], Loss: 0.4404
Epoch [198/200], Loss: 0.3650
Epoch [199/200], Loss: 0.3452
Epoch [200/200], Loss: 0.3958
Train class_to_idx: {'bus_stop_gym': 0, 'bus_stop_xiang_feng': 1, 'ntou_donut': 2, 'ntou_freedom_ship': 3}
Class bus_stop_gym has 5 samples
Class bus_stop_xiang_feng has 5 samples
Class ntou_donut has 6 samples
Class ntou_freedom_ship has 6 samples
Support set size: 12 samples
Query set size: 10 samples
Query 0: True = bus_stop_gym, Predicted = bus_stop_gym
Query 1: True = bus_stop_gym, Predicted = bus_stop_gym
Query 2: True = bus_stop_xiang_feng, Predicted = bus_stop_xiang_feng
Query 3: True = bus_stop_xiang_feng, Predicted = bus_stop_xiang_feng
Query 4: True = ntou_donut, Predicted = ntou_donut
Query 5: True = ntou_donut, Predicted = bus_stop_gym
Query 6: True = ntou_donut, Predicted = ntou_donut
Query 7: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Query 8: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Query 9: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Few-shot prototype classification accuracy: 0.90
Test class_to_idx: {'bus_stop_gym': 0, 'bus_stop_xiang_feng': 1, 'ntou_donut': 2, 'ntou_freedom_ship': 3}
GT: bus_stop_gym, Pred: bus_stop_gym
GT: bus_stop_xiang_feng, Pred: bus_stop_xiang_feng
GT: ntou_donut, Pred: ntou_donut
GT: ntou_freedom_ship, Pred: ntou_freedom_ship
Test accuracy: 1.00

[AutoEval] Running experiments for few_shot_k = 1 to 5

[AutoEval] --- Testing few_shot_k = 1 ---
→ Train acc = 0.89, Test acc = 0.75

[AutoEval] --- Testing few_shot_k = 2 ---
→ Train acc = 0.93, Test acc = 0.75

[AutoEval] --- Testing few_shot_k = 3 ---
→ Train acc = 1.00, Test acc = 0.75

[AutoEval] --- Testing few_shot_k = 4 ---
→ Train acc = 1.00, Test acc = 0.75

[AutoEval] --- Testing few_shot_k = 5 ---
→ Train acc = 1.00, Test acc = 1.00