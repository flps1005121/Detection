Total training samples: 22
Epoch [1/200], Loss: 1.1454
Epoch [2/200], Loss: 0.8367
Epoch [3/200], Loss: 0.6390
Epoch [4/200], Loss: 0.7311
Epoch [5/200], Loss: 0.6419
Epoch [6/200], Loss: 0.4923
Epoch [7/200], Loss: 0.6271
Epoch [8/200], Loss: 0.5299
Epoch [9/200], Loss: 0.5211
Epoch [10/200], Loss: 0.5064
Epoch [11/200], Loss: 0.7123
Epoch [12/200], Loss: 0.7197
Epoch [13/200], Loss: 0.5792
Epoch [14/200], Loss: 0.5121
Epoch [15/200], Loss: 0.5655
Epoch [16/200], Loss: 0.5497
Epoch [17/200], Loss: 0.5225
Epoch [18/200], Loss: 0.7186
Epoch [19/200], Loss: 0.6607
Epoch [20/200], Loss: 0.6149
Epoch [21/200], Loss: 0.6532
Epoch [22/200], Loss: 0.4523
Epoch [23/200], Loss: 0.6356
Epoch [24/200], Loss: 0.5786
Epoch [25/200], Loss: 0.6150
Epoch [26/200], Loss: 0.9640
Epoch [27/200], Loss: 0.5058
Epoch [28/200], Loss: 0.7986
Epoch [29/200], Loss: 0.4970
Epoch [30/200], Loss: 0.4729
Epoch [31/200], Loss: 0.6927
Epoch [32/200], Loss: 0.5244
Epoch [33/200], Loss: 0.5100
Epoch [34/200], Loss: 0.4218
Epoch [35/200], Loss: 0.4976
Epoch [36/200], Loss: 0.4333
Epoch [37/200], Loss: 0.6690
Epoch [38/200], Loss: 0.5352
Epoch [39/200], Loss: 0.3943
Epoch [40/200], Loss: 0.4833
Epoch [41/200], Loss: 0.4728
Epoch [42/200], Loss: 0.4774
Epoch [43/200], Loss: 0.4474
Epoch [44/200], Loss: 0.3926
Epoch [45/200], Loss: 0.4591
Epoch [46/200], Loss: 0.4346
Epoch [47/200], Loss: 0.3737
Epoch [48/200], Loss: 0.4627
Epoch [49/200], Loss: 0.5521
Epoch [50/200], Loss: 0.4408
Epoch [51/200], Loss: 0.4984
Epoch [52/200], Loss: 0.4250
Epoch [53/200], Loss: 0.4273
Epoch [54/200], Loss: 0.5196
Epoch [55/200], Loss: 0.4654
Epoch [56/200], Loss: 0.4244
Epoch [57/200], Loss: 0.7062
Epoch [58/200], Loss: 0.5219
Epoch [59/200], Loss: 0.4989
Epoch [60/200], Loss: 0.6474
Epoch [61/200], Loss: 0.4275
Epoch [62/200], Loss: 0.5418
Epoch [63/200], Loss: 0.3780
Epoch [64/200], Loss: 0.5242
Epoch [65/200], Loss: 0.4933
Epoch [66/200], Loss: 0.3755
Epoch [67/200], Loss: 0.4778
Epoch [68/200], Loss: 0.5968
Epoch [69/200], Loss: 0.3700
Epoch [70/200], Loss: 0.4872
Epoch [71/200], Loss: 0.5036
Epoch [72/200], Loss: 0.4079
Epoch [73/200], Loss: 0.4606
Epoch [74/200], Loss: 0.4385
Epoch [75/200], Loss: 0.5608
Epoch [76/200], Loss: 0.3725
Epoch [77/200], Loss: 0.4080
Epoch [78/200], Loss: 0.4079
Epoch [79/200], Loss: 0.6815
Epoch [80/200], Loss: 0.5202
Epoch [81/200], Loss: 0.4712
Epoch [82/200], Loss: 0.4476
Epoch [83/200], Loss: 0.3666
Epoch [84/200], Loss: 0.4501
Epoch [85/200], Loss: 0.4921
Epoch [86/200], Loss: 0.4341
Epoch [87/200], Loss: 0.3868
Epoch [88/200], Loss: 0.5233
Epoch [89/200], Loss: 0.3777
Epoch [90/200], Loss: 0.5287
Epoch [91/200], Loss: 0.4224
Epoch [92/200], Loss: 0.4881
Epoch [93/200], Loss: 0.5563
Epoch [94/200], Loss: 0.6504
Epoch [95/200], Loss: 0.5254
Epoch [96/200], Loss: 0.5105
Epoch [97/200], Loss: 0.4458
Epoch [98/200], Loss: 0.6917
Epoch [99/200], Loss: 0.4339
Epoch [100/200], Loss: 0.5899
Epoch [101/200], Loss: 0.4177
Epoch [102/200], Loss: 0.5052
Epoch [103/200], Loss: 0.4090
Epoch [104/200], Loss: 0.5268
Epoch [105/200], Loss: 0.6919
Epoch [106/200], Loss: 0.3837
Epoch [107/200], Loss: 0.4269
Epoch [108/200], Loss: 0.4806
Epoch [109/200], Loss: 0.3679
Epoch [110/200], Loss: 0.5653
Epoch [111/200], Loss: 0.5146
Epoch [112/200], Loss: 0.5452
Epoch [113/200], Loss: 0.4611
Epoch [114/200], Loss: 0.4963
Epoch [115/200], Loss: 0.4063
Epoch [116/200], Loss: 0.4554
Epoch [117/200], Loss: 0.4718
Epoch [118/200], Loss: 0.5406
Epoch [119/200], Loss: 0.4237
Epoch [120/200], Loss: 0.4148
Epoch [121/200], Loss: 0.6564
Epoch [122/200], Loss: 0.4462
Epoch [123/200], Loss: 0.5813
Epoch [124/200], Loss: 0.3922
Epoch [125/200], Loss: 0.3694
Epoch [126/200], Loss: 0.4908
Epoch [127/200], Loss: 0.3752
Epoch [128/200], Loss: 0.4819
Epoch [129/200], Loss: 0.3302
Epoch [130/200], Loss: 0.4671
Epoch [131/200], Loss: 0.3889
Epoch [132/200], Loss: 0.4495
Epoch [133/200], Loss: 0.3899
Epoch [134/200], Loss: 0.4941
Epoch [135/200], Loss: 0.3721
Epoch [136/200], Loss: 0.5068
Epoch [137/200], Loss: 0.3415
Epoch [138/200], Loss: 0.4881
Epoch [139/200], Loss: 0.4227
Epoch [140/200], Loss: 0.4622
Epoch [141/200], Loss: 0.4363
Epoch [142/200], Loss: 0.4107
Epoch [143/200], Loss: 0.4106
Epoch [144/200], Loss: 0.3501
Epoch [145/200], Loss: 0.3742
Epoch [146/200], Loss: 0.5689
Epoch [147/200], Loss: 0.4742
Epoch [148/200], Loss: 0.6129
Epoch [149/200], Loss: 0.6080
Epoch [150/200], Loss: 0.4381
Epoch [151/200], Loss: 0.6228
Epoch [152/200], Loss: 0.4786
Epoch [153/200], Loss: 0.6049
Epoch [154/200], Loss: 0.6256
Epoch [155/200], Loss: 0.4083
Epoch [156/200], Loss: 0.4348
Epoch [157/200], Loss: 0.5123
Epoch [158/200], Loss: 0.4555
Epoch [159/200], Loss: 0.4405
Epoch [160/200], Loss: 0.4276
Epoch [161/200], Loss: 0.4750
Epoch [162/200], Loss: 0.3519
Epoch [163/200], Loss: 0.4985
Epoch [164/200], Loss: 0.4018
Epoch [165/200], Loss: 0.4626
Epoch [166/200], Loss: 0.5430
Epoch [167/200], Loss: 0.3791
Epoch [168/200], Loss: 0.5481
Epoch [169/200], Loss: 0.4396
Epoch [170/200], Loss: 0.5500
Epoch [171/200], Loss: 0.6168
Epoch [172/200], Loss: 0.4055
Epoch [173/200], Loss: 0.5618
Epoch [174/200], Loss: 0.5102
Epoch [175/200], Loss: 0.4685
Epoch [176/200], Loss: 0.4435
Epoch [177/200], Loss: 0.4421
Epoch [178/200], Loss: 0.4619
Epoch [179/200], Loss: 0.4150
Epoch [180/200], Loss: 0.4413
Epoch [181/200], Loss: 0.4885
Epoch [182/200], Loss: 0.4363
Epoch [183/200], Loss: 0.4918
Epoch [184/200], Loss: 0.4778
Epoch [185/200], Loss: 0.4412
Epoch [186/200], Loss: 0.3746
Epoch [187/200], Loss: 0.4414
Epoch [188/200], Loss: 0.4092
Epoch [189/200], Loss: 0.4752
Epoch [190/200], Loss: 0.6502
Epoch [191/200], Loss: 0.5856
Epoch [192/200], Loss: 0.4405
Epoch [193/200], Loss: 0.5224
Epoch [194/200], Loss: 0.4329
Epoch [195/200], Loss: 0.4683
Epoch [196/200], Loss: 0.3631
Epoch [197/200], Loss: 0.4767
Epoch [198/200], Loss: 0.3657
Epoch [199/200], Loss: 0.3480
Epoch [200/200], Loss: 0.4090
Train class_to_idx: {'bus_stop_gym': 0, 'bus_stop_xiang_feng': 1, 'ntou_donut': 2, 'ntou_freedom_ship': 3}
Support set size: 12 samples
Query set size: 10 samples
Query 0: True = bus_stop_gym, Predicted = bus_stop_gym
Query 1: True = bus_stop_gym, Predicted = bus_stop_gym
Query 2: True = bus_stop_xiang_feng, Predicted = bus_stop_xiang_feng
Query 3: True = bus_stop_xiang_feng, Predicted = bus_stop_gym
Query 4: True = ntou_donut, Predicted = bus_stop_gym
Query 5: True = ntou_donut, Predicted = ntou_donut
Query 6: True = ntou_donut, Predicted = bus_stop_gym
Query 7: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Query 8: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Query 9: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Few-shot prototype classification accuracy: 0.70
GT: bus_stop_gym, Pred: bus_stop_gym
GT: bus_stop_xiang_feng, Pred: bus_stop_xiang_feng
GT: ntou_donut, Pred: ntou_donut
GT: ntou_freedom_ship, Pred: ntou_freedom_ship
Test accuracy: 1.00

[AutoEval] Running experiments for few_shot_k = 1 to 5

[AutoEval] --- Testing few_shot_k = 1 ---
→ Train acc = 0.89, Test acc = 1.00

[AutoEval] --- Testing few_shot_k = 2 ---
→ Train acc = 0.79, Test acc = 0.75

[AutoEval] --- Testing few_shot_k = 3 ---
→ Train acc = 0.80, Test acc = 1.00

[AutoEval] --- Testing few_shot_k = 4 ---
→ Train acc = 1.00, Test acc = 1.00

[AutoEval] --- Testing few_shot_k = 5 ---
→ Train acc = 1.00, Test acc = 1.00
