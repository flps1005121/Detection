Total training samples: 22
Epoch [1/200], Loss: 1.3011
Epoch [2/200], Loss: 0.8256
Epoch [3/200], Loss: 0.7295
Epoch [4/200], Loss: 0.6788
Epoch [5/200], Loss: 0.8135
Epoch [6/200], Loss: 0.7036
Epoch [7/200], Loss: 0.6889
Epoch [8/200], Loss: 0.6967
Epoch [9/200], Loss: 0.5837
Epoch [10/200], Loss: 0.5654
Epoch [11/200], Loss: 0.5690
Epoch [12/200], Loss: 0.8293
Epoch [13/200], Loss: 0.6087
Epoch [14/200], Loss: 0.5827
Epoch [15/200], Loss: 0.5334
Epoch [16/200], Loss: 0.5350
Epoch [17/200], Loss: 0.5506
Epoch [18/200], Loss: 0.6756
Epoch [19/200], Loss: 0.4565
Epoch [20/200], Loss: 0.6269
Epoch [21/200], Loss: 0.6153
Epoch [22/200], Loss: 0.5259
Epoch [23/200], Loss: 0.5384
Epoch [24/200], Loss: 0.5863
Epoch [25/200], Loss: 0.5181
Epoch [26/200], Loss: 0.6758
Epoch [27/200], Loss: 0.5753
Epoch [28/200], Loss: 0.7469
Epoch [29/200], Loss: 0.6072
Epoch [30/200], Loss: 0.5086
Epoch [31/200], Loss: 0.4674
Epoch [32/200], Loss: 0.4268
Epoch [33/200], Loss: 0.5589
Epoch [34/200], Loss: 0.5173
Epoch [35/200], Loss: 0.4748
Epoch [36/200], Loss: 0.5625
Epoch [37/200], Loss: 0.4580
Epoch [38/200], Loss: 0.5678
Epoch [39/200], Loss: 0.4901
Epoch [40/200], Loss: 0.4965
Epoch [41/200], Loss: 0.4737
Epoch [42/200], Loss: 0.5063
Epoch [43/200], Loss: 0.4028
Epoch [44/200], Loss: 0.3709
Epoch [45/200], Loss: 0.5113
Epoch [46/200], Loss: 0.4787
Epoch [47/200], Loss: 0.5183
Epoch [48/200], Loss: 0.5131
Epoch [49/200], Loss: 0.4092
Epoch [50/200], Loss: 0.4710
Epoch [51/200], Loss: 0.5565
Epoch [52/200], Loss: 0.4390
Epoch [53/200], Loss: 0.5057
Epoch [54/200], Loss: 0.4652
Epoch [55/200], Loss: 0.4965
Epoch [56/200], Loss: 0.7274
Epoch [57/200], Loss: 0.5391
Epoch [58/200], Loss: 0.4748
Epoch [59/200], Loss: 0.6284
Epoch [60/200], Loss: 0.5319
Epoch [61/200], Loss: 0.6483
Epoch [62/200], Loss: 0.6157
Epoch [63/200], Loss: 0.3972
Epoch [64/200], Loss: 0.4534
Epoch [65/200], Loss: 0.3549
Epoch [66/200], Loss: 0.6527
Epoch [67/200], Loss: 0.5059
Epoch [68/200], Loss: 0.4857
Epoch [69/200], Loss: 0.4688
Epoch [70/200], Loss: 0.4901
Epoch [71/200], Loss: 0.5346
Epoch [72/200], Loss: 0.5291
Epoch [73/200], Loss: 0.5986
Epoch [74/200], Loss: 0.5877
Epoch [75/200], Loss: 0.5207
Epoch [76/200], Loss: 0.4424
Epoch [77/200], Loss: 0.3749
Epoch [78/200], Loss: 0.6591
Epoch [79/200], Loss: 0.4323
Epoch [80/200], Loss: 0.5007
Epoch [81/200], Loss: 0.5054
Epoch [82/200], Loss: 0.3649
Epoch [83/200], Loss: 0.5416
Epoch [84/200], Loss: 0.5462
Epoch [85/200], Loss: 0.4546
Epoch [86/200], Loss: 0.4931
Epoch [87/200], Loss: 0.4099
Epoch [88/200], Loss: 0.4212
Epoch [89/200], Loss: 0.5528
Epoch [90/200], Loss: 0.7408
Epoch [91/200], Loss: 0.4908
Epoch [92/200], Loss: 0.4036
Epoch [93/200], Loss: 0.7220
Epoch [94/200], Loss: 0.8823
Epoch [95/200], Loss: 0.5036
Epoch [96/200], Loss: 0.5633
Epoch [97/200], Loss: 0.5615
Epoch [98/200], Loss: 0.5477
Epoch [99/200], Loss: 0.5388
Epoch [100/200], Loss: 0.9396
Epoch [101/200], Loss: 0.5402
Epoch [102/200], Loss: 0.5491
Epoch [103/200], Loss: 0.5646
Epoch [104/200], Loss: 0.4793
Epoch [105/200], Loss: 0.4336
Epoch [106/200], Loss: 0.5683
Epoch [107/200], Loss: 0.6385
Epoch [108/200], Loss: 0.5903
Epoch [109/200], Loss: 0.5859
Epoch [110/200], Loss: 0.4589
Epoch [111/200], Loss: 0.8726
Epoch [112/200], Loss: 0.4976
Epoch [113/200], Loss: 0.4610
Epoch [114/200], Loss: 0.4360
Epoch [115/200], Loss: 0.5571
Epoch [116/200], Loss: 0.4956
Epoch [117/200], Loss: 0.6439
Epoch [118/200], Loss: 0.5398
Epoch [119/200], Loss: 0.4879
Epoch [120/200], Loss: 0.5421
Epoch [121/200], Loss: 0.5641
Epoch [122/200], Loss: 0.5095
Epoch [123/200], Loss: 0.4702
Epoch [124/200], Loss: 0.3864
Epoch [125/200], Loss: 0.3661
Epoch [126/200], Loss: 0.4578
Epoch [127/200], Loss: 0.5963
Epoch [128/200], Loss: 0.5227
Epoch [129/200], Loss: 0.4339
Epoch [130/200], Loss: 0.3770
Epoch [131/200], Loss: 0.4316
Epoch [132/200], Loss: 0.4244
Epoch [133/200], Loss: 0.4573
Epoch [134/200], Loss: 0.4595
Epoch [135/200], Loss: 0.3526
Epoch [136/200], Loss: 0.4462
Epoch [137/200], Loss: 0.3514
Epoch [138/200], Loss: 0.3664
Epoch [139/200], Loss: 0.5193
Epoch [140/200], Loss: 0.4751
Epoch [141/200], Loss: 0.3670
Epoch [142/200], Loss: 0.3579
Epoch [143/200], Loss: 0.6082
Epoch [144/200], Loss: 0.5580
Epoch [145/200], Loss: 0.3952
Epoch [146/200], Loss: 0.5833
Epoch [147/200], Loss: 0.6135
Epoch [148/200], Loss: 0.6443
Epoch [149/200], Loss: 0.3770
Epoch [150/200], Loss: 0.4979
Epoch [151/200], Loss: 0.5788
Epoch [152/200], Loss: 0.4752
Epoch [153/200], Loss: 0.4676
Epoch [154/200], Loss: 0.4130
Epoch [155/200], Loss: 0.6159
Epoch [156/200], Loss: 0.3615
Epoch [157/200], Loss: 0.3528
Epoch [158/200], Loss: 0.4461
Epoch [159/200], Loss: 0.3984
Epoch [160/200], Loss: 0.4717
Epoch [161/200], Loss: 0.4442
Epoch [162/200], Loss: 0.3796
Epoch [163/200], Loss: 0.5193
Epoch [164/200], Loss: 0.3529
Epoch [165/200], Loss: 0.3687
Epoch [166/200], Loss: 0.5096
Epoch [167/200], Loss: 0.4726
Epoch [168/200], Loss: 0.5078
Epoch [169/200], Loss: 0.4066
Epoch [170/200], Loss: 0.4662
Epoch [171/200], Loss: 0.6481
Epoch [172/200], Loss: 0.3944
Epoch [173/200], Loss: 0.6932
Epoch [174/200], Loss: 0.4718
Epoch [175/200], Loss: 0.4937
Epoch [176/200], Loss: 0.3749
Epoch [177/200], Loss: 0.4459
Epoch [178/200], Loss: 0.4056
Epoch [179/200], Loss: 0.4286
Epoch [180/200], Loss: 0.4301
Epoch [181/200], Loss: 0.4701
Epoch [182/200], Loss: 0.4705
Epoch [183/200], Loss: 0.3863
Epoch [184/200], Loss: 0.3309
Epoch [185/200], Loss: 0.3401
Epoch [186/200], Loss: 0.4501
Epoch [187/200], Loss: 0.4399
Epoch [188/200], Loss: 0.4184
Epoch [189/200], Loss: 0.3859
Epoch [190/200], Loss: 0.3920
Epoch [191/200], Loss: 0.3834
Epoch [192/200], Loss: 0.4272
Epoch [193/200], Loss: 0.3441
Epoch [194/200], Loss: 0.3594
Epoch [195/200], Loss: 0.3496
Epoch [196/200], Loss: 0.4088
Epoch [197/200], Loss: 0.3681
Epoch [198/200], Loss: 0.3561
Epoch [199/200], Loss: 0.4068
Epoch [200/200], Loss: 0.3902
Train class_to_idx: {'bus_stop_gym': 0, 'bus_stop_xiang_feng': 1, 'ntou_donut': 2, 'ntou_freedom_ship': 3}
Support set size: 12 samples
Query set size: 10 samples
Query 0: True = bus_stop_gym, Predicted = bus_stop_gym
Query 1: True = bus_stop_gym, Predicted = ntou_donut
Query 2: True = bus_stop_xiang_feng, Predicted = bus_stop_xiang_feng
Query 3: True = bus_stop_xiang_feng, Predicted = bus_stop_xiang_feng
Query 4: True = ntou_donut, Predicted = ntou_donut
Query 5: True = ntou_donut, Predicted = ntou_donut
Query 6: True = ntou_donut, Predicted = ntou_donut
Query 7: True = ntou_freedom_ship, Predicted = bus_stop_xiang_feng
Query 8: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Query 9: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Few-shot prototype classification accuracy: 0.80
GT: bus_stop_gym, Pred: bus_stop_gym
GT: bus_stop_xiang_feng, Pred: bus_stop_xiang_feng
GT: ntou_donut, Pred: ntou_donut
GT: ntou_freedom_ship, Pred: ntou_freedom_ship
Test accuracy: 1.00

[AutoEval] Running experiments for few_shot_k = 1 to 5

[AutoEval] --- Testing few_shot_k = 1 ---
→ Train acc = 0.83, Test acc = 1.00

[AutoEval] --- Testing few_shot_k = 2 ---
→ Train acc = 1.00, Test acc = 1.00

[AutoEval] --- Testing few_shot_k = 3 ---
→ Train acc = 0.90, Test acc = 0.75

[AutoEval] --- Testing few_shot_k = 4 ---
→ Train acc = 0.83, Test acc = 0.75

[AutoEval] --- Testing few_shot_k = 5 ---
→ Train acc = 1.00, Test acc = 1.00
