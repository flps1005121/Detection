Total training samples: 22
Epoch [1/200], Loss: 1.0931
Epoch [2/200], Loss: 0.8525
Epoch [3/200], Loss: 0.7688
Epoch [4/200], Loss: 0.8278
Epoch [5/200], Loss: 0.8133
Epoch [6/200], Loss: 0.6332
Epoch [7/200], Loss: 0.6275
Epoch [8/200], Loss: 0.6489
Epoch [9/200], Loss: 0.9485
Epoch [10/200], Loss: 0.7166
Epoch [11/200], Loss: 0.7701
Epoch [12/200], Loss: 0.5855
Epoch [13/200], Loss: 0.7097
Epoch [14/200], Loss: 0.6835
Epoch [15/200], Loss: 0.8450
Epoch [16/200], Loss: 0.4676
Epoch [17/200], Loss: 0.6510
Epoch [18/200], Loss: 0.9989
Epoch [19/200], Loss: 0.6575
Epoch [20/200], Loss: 0.6815
Epoch [21/200], Loss: 0.7916
Epoch [22/200], Loss: 1.0059
Epoch [23/200], Loss: 0.7057
Epoch [24/200], Loss: 0.5899
Epoch [25/200], Loss: 0.5752
Epoch [26/200], Loss: 0.6341
Epoch [27/200], Loss: 0.6556
Epoch [28/200], Loss: 0.6827
Epoch [29/200], Loss: 0.6763
Epoch [30/200], Loss: 0.7750
Epoch [31/200], Loss: 0.7575
Epoch [32/200], Loss: 0.6457
Epoch [33/200], Loss: 0.4564
Epoch [34/200], Loss: 0.4986
Epoch [35/200], Loss: 0.7708
Epoch [36/200], Loss: 0.6811
Epoch [37/200], Loss: 1.2079
Epoch [38/200], Loss: 1.0997
Epoch [39/200], Loss: 0.8439
Epoch [40/200], Loss: 0.8125
Epoch [41/200], Loss: 0.6152
Epoch [42/200], Loss: 0.6721
Epoch [43/200], Loss: 0.6325
Epoch [44/200], Loss: 0.7402
Epoch [45/200], Loss: 0.9703
Epoch [46/200], Loss: 0.7836
Epoch [47/200], Loss: 0.9049
Epoch [48/200], Loss: 0.6956
Epoch [49/200], Loss: 0.5560
Epoch [50/200], Loss: 0.6735
Epoch [51/200], Loss: 0.6845
Epoch [52/200], Loss: 0.6421
Epoch [53/200], Loss: 0.4913
Epoch [54/200], Loss: 0.6689
Epoch [55/200], Loss: 0.5786
Epoch [56/200], Loss: 0.3938
Epoch [57/200], Loss: 0.6076
Epoch [58/200], Loss: 0.6169
Epoch [59/200], Loss: 0.6605
Epoch [60/200], Loss: 0.4130
Epoch [61/200], Loss: 0.4242
Epoch [62/200], Loss: 0.6694
Epoch [63/200], Loss: 0.4113
Epoch [64/200], Loss: 0.5734
Epoch [65/200], Loss: 0.7703
Epoch [66/200], Loss: 0.6768
Epoch [67/200], Loss: 0.7718
Epoch [68/200], Loss: 0.6514
Epoch [69/200], Loss: 0.7136
Epoch [70/200], Loss: 0.4718
Epoch [71/200], Loss: 0.4343
Epoch [72/200], Loss: 0.6482
Epoch [73/200], Loss: 0.5362
Epoch [74/200], Loss: 0.6560
Epoch [75/200], Loss: 0.5739
Epoch [76/200], Loss: 0.5013
Epoch [77/200], Loss: 0.4942
Epoch [78/200], Loss: 0.6996
Epoch [79/200], Loss: 0.4608
Epoch [80/200], Loss: 0.5256
Epoch [81/200], Loss: 0.5986
Epoch [82/200], Loss: 0.4759
Epoch [83/200], Loss: 0.5354
Epoch [84/200], Loss: 0.5476
Epoch [85/200], Loss: 0.7786
Epoch [86/200], Loss: 0.4857
Epoch [87/200], Loss: 0.5783
Epoch [88/200], Loss: 0.4345
Epoch [89/200], Loss: 1.1626
Epoch [90/200], Loss: 0.5710
Epoch [91/200], Loss: 0.6256
Epoch [92/200], Loss: 0.4865
Epoch [93/200], Loss: 0.5857
Epoch [94/200], Loss: 0.5841
Epoch [95/200], Loss: 0.5326
Epoch [96/200], Loss: 0.4242
Epoch [97/200], Loss: 0.6521
Epoch [98/200], Loss: 0.4220
Epoch [99/200], Loss: 0.3856
Epoch [100/200], Loss: 0.4437
Epoch [101/200], Loss: 0.4876
Epoch [102/200], Loss: 0.6652
Epoch [103/200], Loss: 0.6388
Epoch [104/200], Loss: 0.6566
Epoch [105/200], Loss: 0.6123
Epoch [106/200], Loss: 0.4432
Epoch [107/200], Loss: 0.5758
Epoch [108/200], Loss: 0.6036
Epoch [109/200], Loss: 0.4773
Epoch [110/200], Loss: 0.4368
Epoch [111/200], Loss: 0.4422
Epoch [112/200], Loss: 0.5361
Epoch [113/200], Loss: 0.4517
Epoch [114/200], Loss: 0.5554
Epoch [115/200], Loss: 0.4865
Epoch [116/200], Loss: 0.5324
Epoch [117/200], Loss: 0.5738
Epoch [118/200], Loss: 0.4563
Epoch [119/200], Loss: 0.4309
Epoch [120/200], Loss: 0.4952
Epoch [121/200], Loss: 0.3792
Epoch [122/200], Loss: 0.4685
Epoch [123/200], Loss: 0.5121
Epoch [124/200], Loss: 0.5840
Epoch [125/200], Loss: 0.5257
Epoch [126/200], Loss: 0.4995
Epoch [127/200], Loss: 0.5000
Epoch [128/200], Loss: 0.4249
Epoch [129/200], Loss: 0.4437
Epoch [130/200], Loss: 0.5670
Epoch [131/200], Loss: 0.6037
Epoch [132/200], Loss: 0.5422
Epoch [133/200], Loss: 0.4089
Epoch [134/200], Loss: 0.4723
Epoch [135/200], Loss: 0.4732
Epoch [136/200], Loss: 0.4260
Epoch [137/200], Loss: 0.5298
Epoch [138/200], Loss: 0.5566
Epoch [139/200], Loss: 0.3716
Epoch [140/200], Loss: 0.4846
Epoch [141/200], Loss: 0.5149
Epoch [142/200], Loss: 0.5848
Epoch [143/200], Loss: 0.5738
Epoch [144/200], Loss: 0.4320
Epoch [145/200], Loss: 0.3760
Epoch [146/200], Loss: 0.5362
Epoch [147/200], Loss: 0.3934
Epoch [148/200], Loss: 0.6928
Epoch [149/200], Loss: 0.4575
Epoch [150/200], Loss: 0.4365
Epoch [151/200], Loss: 0.5718
Epoch [152/200], Loss: 0.4883
Epoch [153/200], Loss: 0.4843
Epoch [154/200], Loss: 0.3460
Epoch [155/200], Loss: 0.4502
Epoch [156/200], Loss: 0.5207
Epoch [157/200], Loss: 0.3873
Epoch [158/200], Loss: 0.5159
Epoch [159/200], Loss: 0.5681
Epoch [160/200], Loss: 0.4066
Epoch [161/200], Loss: 0.5292
Epoch [162/200], Loss: 0.4237
Epoch [163/200], Loss: 0.3748
Epoch [164/200], Loss: 0.4791
Epoch [165/200], Loss: 0.4370
Epoch [166/200], Loss: 0.6432
Epoch [167/200], Loss: 0.4135
Epoch [168/200], Loss: 0.4951
Epoch [169/200], Loss: 0.5189
Epoch [170/200], Loss: 0.4252
Epoch [171/200], Loss: 0.5657
Epoch [172/200], Loss: 0.5109
Epoch [173/200], Loss: 0.4439
Epoch [174/200], Loss: 0.4370
Epoch [175/200], Loss: 0.5348
Epoch [176/200], Loss: 0.4317
Epoch [177/200], Loss: 0.3699
Epoch [178/200], Loss: 0.3735
Epoch [179/200], Loss: 0.4218
Epoch [180/200], Loss: 0.4599
Epoch [181/200], Loss: 0.4546
Epoch [182/200], Loss: 0.3747
Epoch [183/200], Loss: 0.4420
Epoch [184/200], Loss: 0.4766
Epoch [185/200], Loss: 0.3840
Epoch [186/200], Loss: 0.3364
Epoch [187/200], Loss: 0.3899
Epoch [188/200], Loss: 0.4892
Epoch [189/200], Loss: 0.4056
Epoch [190/200], Loss: 0.3911
Epoch [191/200], Loss: 0.6703
Epoch [192/200], Loss: 0.4148
Epoch [193/200], Loss: 0.5124
Epoch [194/200], Loss: 0.4743
Epoch [195/200], Loss: 0.4542
Epoch [196/200], Loss: 0.4732
Epoch [197/200], Loss: 0.3638
Epoch [198/200], Loss: 0.4202
Epoch [199/200], Loss: 0.4251
Epoch [200/200], Loss: 0.4386
Train class_to_idx: {'bus_stop_gym': 0, 'bus_stop_xiang_feng': 1, 'ntou_donut': 2, 'ntou_freedom_ship': 3}
Support set size: 12 samples
Query set size: 10 samples
Query 0: True = bus_stop_gym, Predicted = bus_stop_xiang_feng
Query 1: True = bus_stop_gym, Predicted = bus_stop_gym
Query 2: True = bus_stop_xiang_feng, Predicted = bus_stop_xiang_feng
Query 3: True = bus_stop_xiang_feng, Predicted = bus_stop_xiang_feng
Query 4: True = ntou_donut, Predicted = ntou_donut
Query 5: True = ntou_donut, Predicted = ntou_donut
Query 6: True = ntou_donut, Predicted = ntou_donut
Query 7: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Query 8: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Query 9: True = ntou_freedom_ship, Predicted = ntou_freedom_ship
Few-shot prototype classification accuracy: 0.90
GT: bus_stop_gym, Pred: bus_stop_gym
GT: bus_stop_xiang_feng, Pred: bus_stop_xiang_feng
GT: ntou_donut, Pred: ntou_donut
GT: ntou_freedom_ship, Pred: ntou_freedom_ship
Test accuracy: 1.00

[AutoEval] Running experiments for few_shot_k = 1 to 5

[AutoEval] --- Testing few_shot_k = 1 ---
→ Train acc = 0.78, Test acc = 0.50

[AutoEval] --- Testing few_shot_k = 2 ---
→ Train acc = 0.79, Test acc = 0.75

[AutoEval] --- Testing few_shot_k = 3 ---
→ Train acc = 0.90, Test acc = 1.00

[AutoEval] --- Testing few_shot_k = 4 ---
→ Train acc = 1.00, Test acc = 1.00

[AutoEval] --- Testing few_shot_k = 5 ---
→ Train acc = 1.00, Test acc = 1.00
